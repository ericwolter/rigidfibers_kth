\chapter{Benchmarks}
\label{cha:benchmarks}

The previous chapter introduced the parallel implementation of the numerical algorithm using NVIDIA's CUDA framework. It presented a practical overview of the GPU implementation and discussed various strategies for optimizing the algorithm.

In this chapter we will present a number of benchmark tests. The main goal of the benchmarks is to measure the performance of the GPU implementation and compare it to the performance of the OpenMP-based CPU implementation. In addition to the comparison between the two different implementations, benchmark tests are also performed to investigate the different approaches for optimizing the code presented in the previous chapter.

\section{Methodology}

The methodology used for the benchmark suite is the same for all presented benchmarks. This ensures comparable results and the fairest comparison possible. A brief overview of the hardware and benchmark scheme used are described in the next sections.

\subsection{Hardware}

All benchmarks are run on the same workstation with specifications listed in Tab.~\ref{tab:workstation}. These hardware components can be considered a balanced system. This is done to come as close as possible to a fair comparison between the CPU and GPU, as comparing a high-end CPU against a low-end GPU would only be of limited value.

\begin{table}[htbp]
  \begin{center}
    \begin{tabulary}{0.7\textwidth}{LL}
      \toprule
      \multicolumn{2}{c}{Workstation} \\
      \midrule
      Processor & Intel Core i7 4770 \\
      Graphics & NVIDIA GTX 970 4GB \\
      RAM & 16GB DDR3 \\
      Operating System & Ubuntu Linux 12.04 LTS \\
      CUDA Driver & CUDA 6.5.16 \\
      \bottomrule
    \end{tabulary}
  \end{center}
  \caption{Benchmark hardware specification.}
  \label{tab:workstation}
\end{table}

The Intel Core i7 4770 processor is an 4-Core CPU based on Intel's Haswell Architecture. With its 8 parallel threads and $3.4$ GHz it was one of the top-of-line processor from 2013/14 and is currently still available for around $300\$$. The NVIDIA GTX 970 4GB is part of NVIDIA newest lineup of graphics cards based on the Maxwell Architecture. The main advantage of these new cards is the large memory of $4$ GB allowing for larger simulations. With a current price of slightly above $300\$$ it fills the middle price class for all Maxwell cards. Overall this can be considered to be a balanced system.

\subsection{Benchmark scheme}
\label{subsec:benchmark_scheme}

In order to generate statistically significant and reproducible execution times, all benchmarks tests for both the CPU and GPU implementation uses exactly the same run-scheme.

For each benchmark run we start with a configuration of M fibers randomly distributed in space. In order to exclude configurations where fibers overlap and/or intersect, the random process is modified such that there always is a fixed minimum distance between two fibers. Furthermore, in order to ensure a fair comparison between e.g. different linear solvers, the average distance between fibers are kept fixed for all runs independently of the number of fibers. As will be shown in Sec.~\ref{subsec:example_concentration_gmres}, the number of iterations of the iterative solvers depend on the average distance between the fibers. As the average distance become smaller, the condition number of the matrix increases and more iterations are needed for the iterative solver to converge. Thus by keeping the average distance between the fibers fixed, we minimize the influence of the number of iterations in the iterative solver on the execution time.

Using the semi-random fiber configuration, the simulation is run for 10 time steps. To avoid remaining outliers in the configuration potentially causing variations in the timings the first time step is excluded. The first time step is thus used as a simple warmup step for the simulation. So, the final average time for each run is measured using the final $9$ time steps.

To measure how the execution time depends on the number of fibers in the simulation, all tests are run with varying number of fibers starting from $100$ fibers up to $2000$ fibers using an increment of $100$.

\begin{listing}[htbp]
  \centering
  \inputminted[mathescape,
    linenos,
    numbersep=5pt,
    fontsize=\footnotesize,
    frame=lines,
    framesep=2mm]{c}{lst/benchmark_scheme.lst}
  \caption{Pseudocode for benchmark scheme.}
  \label{lst:pseudo_benchmark}
\end{listing}

In addition, every run is repeated a number of times where each run uses a new random fiber configuration. The final execution time is computed as the average time over the total number of runs. The execution time is measured using the built-in CUDA timing events for the GPU implementation and the Fortran \emph{SYSTEM\_CLOCK} function for the CPU implementation.

To further improve the statistical significance of the result the benchmark scheme dynamically adjusts the number of runs performed for each test. If the relative standard error (RSE) of the measured timings collected is too high after a minimum number of runs, more runs are scheduled. This repeats until the relative standard error falls below $20\%$ and reliable timings have been obtained.  The algorithm for producing the benchmark results is illustrated using pseudocode in Listing~\ref{lst:pseudo_benchmark}. 

\section{Optimizations}
\label{sec:bench_optimization}

We now look at the performance results for the different optimizations strategies previously outlined in Sec.~\ref{sec:parallel_optimizations}. Where applicable, the results will be compared between the OpenMP and the CUDA version of the algorithm.

\subsection{Numeric vs. Analytic Integration}
\label{subsec:bench_numeric_vs_analytic}

The first benchmark tests the performance of the two different approaches to compute the inner integral in Eqn.~\eqref{eq:inner_integral}. It can be solved either numerically or analytically. Fig.~\ref{fig:openmp_num_vs_anal} illustrates the performance timings for the \emph{Assemble System} step of the parallel OpenMP version. Inline with the observations made by the authors of the original serial implementation,~\cite{Tornberg2006}, analytical integration is always faster to use than numerical integration.
\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
	width=0.77\textwidth,
      xlabel={Number of fibers},
      ylabel={Execution time (sec)},
      enlarge y limits=true,
      enlarge x limits=true,
      unbounded coords=discard,
      xmode=log,
      ymode=log,
      grid=major,
      ]
      \addplot[color=set11,mark=*,mark options={fill=white}, very thick] table[x=X,y=ASSEMBLE_SYSTEM] {benchmarks/openmp_direct_numerical.csv};
      \addplot[color=set12,mark=square*,mark options={fill=white}, very thick] table[x=X,y=ASSEMBLE_SYSTEM] {benchmarks/openmp_direct_analytical.csv};

      \legend{Numerical, Analytical}
    \end{axis}
  \end{tikzpicture}
  \caption[Benchmark computing inner integral on CPU.]{Benchmark comparing numerical and analytical integration of the inner integral in Eqn.~\eqref{eq:inner_integral} using OpenMP.}
  \label{fig:openmp_num_vs_anal}

  \begin{tikzpicture}
    \begin{axis}[
	width=0.77\textwidth,
      xlabel={Number of fibers},
      ylabel={Execution time (sec)},
      enlarge y limits=true,
      enlarge x limits=true,
      unbounded coords=discard,
      xmode=log,
      ymode=log,
      grid=major,
      ]
      \addplot[color=set11,mark=*,mark options={fill=white}, very thick] table[x=X,y=assemble_system] {benchmarks/cuda_bicgstab_numerical_2D.csv};
      \addplot[color=set12,mark=square*,mark options={fill=white}, very thick] table[x=X,y=assemble_system] {benchmarks/cuda_bicgstab_analytical_2D.csv};

      \legend{Numerical, Analytical}
    \end{axis}
  \end{tikzpicture}
  \caption[Benchmark computing inner integral on GPU.]{Benchmark comparing numerical and analytical integration of the inner integral in Eqn.~\eqref{eq:inner_integral} using CUDA.}
  \label{fig:cuda_num_vs_anal}

\end{figure}

In contrast to the expected and obtained results using the OpenMP implementation, the CUDA implementation shows a different picture. When we look at the corresponding graph for the CUDA implementation in Fig.~\ref{fig:cuda_num_vs_anal}, we observe that the results are reversed. Numerical integration outperforms the analytical integration by a large margin that even increases with the number of fibers.

The reason for this result lies in the scheduling and execution of work on the GPU. All code inside a thread block (more precisely a warp) is always executed in lockstep. This means that each line of code is executed for each thread in parallel. However, if the code encounters a branch in the execution path, like a simple \emph{if} statement, the threads diverge. First, all threads for which the condition is true are executed while the other threads have to wait. Then all threads for which the condition is false are executed while the others are not used. Finally, after all divergent code paths have been executed the code continues in lockstep. This issue is referred to as branch divergence and should be avoided as much as possible when writing parallel GPU Code,~\cite{CudaBestPracticeGuide}.

To confirm that branch divergence is the reason for the slowdown of the analytic integration version of the GPU implementation we look at the metrics of the CUDA profiler \emph{nvprof}. The metric \emph{Warp Execution Efficency} shows the ratio of the average active threads per warp to the maximum number of threads per warp. The metrics for both the numerical and analytical integration can be seen in Tab.~\ref{tab:branch_divergence}.

\begin{table}[htbp]
  \begin{center}
    \begin{tabulary}{0.7\textwidth}{LR}
      \toprule
      Algorithm & warp\_execution\_efficiency \\
      \midrule
      Numerical & $99.01\%$ \\
      Analytical & $53.79\%$ \\
      \bottomrule
    \end{tabulary}
  \end{center}
  \caption[Warp Execution Efficiency of Numerical vs. Analytical Integration.]{CUDA performance metric \emph{Warp Exection Efficiency} comparison for the numerical and analytical integration of the inner integral in Eqn.~\eqref{eq:inner_integral}.}
  \label{tab:branch_divergence}
\end{table}

On the one hand the numerical integration is almost $100\%$ efficient, which means that all warps execute in complete lockstep. The analytical integration on the other hand is only $50\%$ efficient, meaning that most of the time only half of the threads actually perform work while the other half is just waiting. This results in the observed performance difference. As already discussed in Sec.~\ref{subsec:numeric_analytic} the analytical evaluation of the inner integral potentially suffers from numerical instabilities. Closer inspection of the source code reveals that the steps taken to minimize these instabilities are responsible for the branch divergence and explains the decrease in performance on the GPU. The steps involve a simple \emph{if} statement, that switches between two code path depending on how far apart two fibers are. Unfortunately, this workaround is unavoidable to ensure numeric stability.

\subsection{Shared memory}
\label{subsec:bench_shared_memory}

Since the data transfer between the compute units and the global memory is slow, the second optimization strategy is to try to use shared memory to reduce the amount of data that has to be transferred. For this each Streaming Multiprocessor (SM) has a small amount of locally shared memory. This memory can be accessed from all threads on the SM. If data can be shared, it only has to be transferred from global to shared memory once. Afterwards, in can be accessed from the faster local memory.

During testing and benchmarking the shared memory implementation of the \emph{Assemble System} step described in Sec.~\ref{subsec:shared_memory}, showed no effect on the performance. Even though data can theoretically be shared among different threads it does not result in shorter execution times.

The reason for this is that the \emph{Assemble System} step is compute bound and not memory bound. This means that the time it takes to execute the computations, e.g. evaluating the integrals, takes substantially longer than reading and writing to global memory. This can be explained by looking at the two-dimensional thread block version, where each kernel is responsible for a pair of fibers. Each kernel only needs to read $4$ vectors from global memory, the position and orientation of both fibers. Assuming single precision this is a total of just $4 * 3 * 4~\text{bytes} = 48~\text{bytes}$ per kernel invocation.

While waiting for this data from global memory, CUDA is able to quickly switch between different sets of threads and continue the computation there. Thus the only waiting time occurs when the very first set of data has to be loaded. Once the first amount of data has arrived, computations can be performed using it. While the long running computations are executed, other threads can start issuing data loading requests. When the first computation is done and the next set of threads is executed, the data has already been read from memory. As there was no advantage in using shared memory in the compute bound \emph{Assemble System} step, we opted for the simpler implementation without it in all subsequent simulations.

\subsection{Thread block dimension}
\label{subsec:bench_thread_block}

The final optimization strategy studied is the Thread Block Dimension on the GPU. Choosing the best option is a trade-off between the resources used and the overhead caused by an increased amount of memory writes to the same location. Writing to the same memory location from different threads would result in undefined behavior and avoiding this requires the usage of potentially slow atomic functions.

The results in Fig.~\ref{fig:bench_cuda_thread_blocks} indicate that the best option for this particular GPU is a two-dimensional thread block. The three-dimensional thread block is always slower and the performance gap grows with the number of fibers. The reason for this performance gap is the increased usage of atomics in the three-dimensional case. The overall usage of atomic functions can be inspected with the NVIDIA profiler \emph{nvprof} and the profiling metric \emph{Atomic Transactions}. This metric simply counts the total number of atomic transactions performed when atomic functions are used. Tab.~\ref{tab:atomic_transactions} lists the \emph{Atomic Transactions} counts for both the two-dimensional and three-dimensional thread block implementation running an example with $2000$ fibers. The required \emph{Atomic Transactions} in the three-dimensional case are almost two times larger than for the two-dimensional case. These additional transactions incur a performance penalty, because they serialize the access to memory and threads have to wait while other threads finish writing to memory.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      xlabel={Number of fibers},
      ylabel={Execution time (sec)},
      enlarge y limits=true,
      enlarge x limits=true,
      unbounded coords=discard,
      xmode=log,
      ymode=log,
      grid=major,
      ]
      \addplot[color=set11,mark=*,mark options={fill=white}, very thick] table[x=X,y=assemble_system] {benchmarks/cuda_magma_numerical_1D.csv};
      \addplot[color=set12,mark=square*,mark options={fill=white}, very thick] table[x=X,y=assemble_system] {benchmarks/cuda_magma_numerical_2D.csv};
      \addplot[color=set13,mark=triangle*,mark options={fill=white}, very thick] table[x=X,y=assemble_system] {benchmarks/cuda_magma_numerical_3D.csv};

      \legend{1D, 2D, 3D}
    \end{axis}
  \end{tikzpicture}
  \caption[Benchmarking thread block dimensions.]{Benchmark comparing different parallelization options of the \emph{Assemble System} step using thread block dimensions as described in Sec.~\ref{subsec:parallel_thread_block}. }
  \label{fig:bench_cuda_thread_blocks}
\end{figure}

\begin{table}[htbp]
  \begin{center}
    \begin{tabulary}{0.7\textwidth}{LR}
      \toprule
      Algorithm & atomic\_transactions \\
      \midrule
      2D & $1,269,325$ \\
      3D & $2,350,670$ \\
      \bottomrule
    \end{tabulary}
  \end{center}
  \caption[Atomic transactions of 2D vs. 3D thread block dimensions.]{CUDA performance metric \emph{Atomic transactions} comparison for the 2D and 3D thread block dimensions parallelization of the \emph{Assemble System} step.}
  \label{tab:atomic_transactions}
\end{table}

Fig.~\ref{fig:bench_cuda_thread_blocks} also shows that the one-dimensional approach is slower than both the two-dimensional and three-dimensional approach. However, it appears to scale linearly whereas the other two scale quadratically with the number of fibers. It can already be observed that the performance of the one-dimensional thread block becomes faster than the three-dimensional thread block for close to $2000$ fibers. Unfortunately, the hardware of the workstation does not have enough memory to simulate more fibers, allowing the one-dimensional approach to overtake the two-dimensional approach. Thus at least for our simulation we always use the two-dimensional approach.

\section{Linear solvers}
\label{sec:bench_linear_solvers}

Next we compare the performance for different linear solvers. The time required for solving the linear system can be a very large part of the overall runtime, depending choice of solver and the fiber configuration as discussed in Sec.~\ref{sec:algorithm_summary}. It is therefore very important to find the optimal solver in order to arrive at the best performing algorithm overall. We explore both direct and iterative solvers.

\subsection{Direct solver vs iterative solver on CPU}
On the CPU side we use the direct solver provided by the fully parallelized OpenBLAS library. For GMRES we use the single precision Fortran implementation from Frayssé et al.,~\cite{Fraysse2003}, which takes extensive advantage of the underlying BLAS functions parallelized by OpenBLAS. 

If we compare the execution time when using GMRES to using a direct solver, we observe that GMRES is faster by a wide margin, see Fig.~\ref{fig:bench_openmp_solvers}. This was also observed in the original serial version of the code,~\cite{Tornberg2006}.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      xlabel={Number of fibers},
      ylabel={Execution time (sec)},
      enlarge y limits=true,
      enlarge x limits=true,
      unbounded coords=discard,
      xmode=log,
      ymode=log,
      grid=major,
      ]
      \addplot[color=set11,mark=*,mark options={fill=white}, very thick] table[x=X,y=SOLVE_SYSTEM] {benchmarks/openmp_direct_numerical.csv};
      \addplot[color=set12,mark=square*,mark options={fill=white}, very thick] table[x=X,y=SOLVE_SYSTEM] {benchmarks/openmp_gmres_numerical.csv};

      \legend{Direct Solver, GMRES}
    \end{axis}
  \end{tikzpicture}
  \caption[Benchmark linear solvers on CPU.]{Benchmarking comparing linear solvers on the CPU. The direct solver is provided by OpenBLAS,~\cite{OpenBLAS}, and iterative GMRES solver by Frayssé et al.,~\cite{Fraysse2003}.}
  \label{fig:bench_openmp_solvers}
\end{figure}

From an advantage of just ${\sim}40×$ for $1000$ fibers this increases to ${\sim}300×$ for the maximum number of $2000$ fibers. As expected we can also see in Fig~\ref{fig:bench_openmp_solvers} that GMRES clearly scales better with the number of fibers than the direct solver.

\subsection{Fiber concentration effect on GMRES iterations}
\label{subsec:example_concentration_gmres}

During the initial benchmark tests we observed large differences in the execution time for different fiber configurations when using GMRES. The reason for this was that for some runs, especially for a high concentration of fibers, GMRES required a large number of iterations to converge.

When the fiber concentration increases, the average distance between the fibers decreases. To investigate how the number of GMRES iterations depend on the concentration of fibers, we perform a number of runs where the average pair-wise distance between the fibers varies from $0.01$ to $40$.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      xlabel={Concentration},
      ylabel={GMRES iterations},
      width={0.8\textwidth},
      unbounded coords=discard,
      xmode=log,
      ymode=log,
      grid=major,
      enlarge y limits=true,
      enlarge x limits=true,
      ]

      \addplot[color=set11, mark=*,mark options={fill=white}, very thick] table[x=Concentration,y=Average,col sep=comma] {charts/concentration_gmres.csv};

    \end{axis}
  \end{tikzpicture}
  \caption[Effect of fiber concentration on GMRES iterations.]{Effect of fiber concentration on GMRES iterations. The number of GMRES iterations increases dramatically when the distance between the fibers is small.}
  \label{fig:concentration_gmres}
\end{figure}

In Fig.~\ref{fig:concentration_gmres} the result is presented. We see that for a decrease in the pair-wise distance, the number of iterations increases quite rapidly. To find the reason for this we have to look at Eqn.~\eqref{eq:inner_integral} which forms the basis for the entries in the matrix. When evaluating the integral the Greens function $\mathbf{G}(\mathbf{R})$ defined in Eqn.~\eqref{eq:green_function} has to be computed. Since $\mathbf{G}(\mathbf{R}) \sim 1/\mathbf{R}$ where $\mathbf{R}$ is given by the distance between fibers, some of the entries in the matrix become very large when the distance is small. The consequence is a growth in the condition number of the matrix and thus an increase in the number of GMRES iterations is required for convergence. This is the reason why we keep the concentration of fibers fixed for all benchmark runs, as described in Sec.~\ref{subsec:benchmark_scheme}.

The result presented above is also important to keep in mind when performing long running fiber simulations. Here, the probability that any two fibers get close to each other is very high. If this is the case, solving the system by GMRES will take longer than expected. In practice it might be more beneficial to switch to the direct solver as it has a predictable runtime. This is especially true using the GPU implementation as the difference in execution time between direct and iterative solver is relatively small. This will be discussed further in the next section.

\subsection{Direct solver vs. iterative solver on GPU}

We will now look at the performance of the direct solver compared to the iterative solvers on the GPU. We use a direct solver provided by the MAGMA library and the iterative solvers, BiCGStab and GMRES, from the ViennaCL library. The benchmark results are illustrated in Fig.~\ref{fig:bench_cuda_solvers}.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      xlabel={Number of fibers},
      ylabel={Execution time (sec)},
      enlarge y limits=true,
      enlarge x limits=true,
      unbounded coords=discard,
      xmode=log,
      ymode=log,
      grid=major,
      ]
      \addplot[color=set11,mark=*,mark options={fill=white}, very thick] table[x=X,y=solve_system] {benchmarks/cuda_magma_numerical_2D.csv};
      \addplot[color=set12,mark=square*,mark options={fill=white}, very thick] table[x=X,y=solve_system] {benchmarks/cuda_bicgstab_numerical_2D.csv};
      \addplot[color=set13,mark=triangle*,mark options={fill=white}, very thick] table[x=X,y=solve_system] {benchmarks/cuda_gmres_numerical_2D.csv};

      \legend{Direct Solver, BiCGStab, GMRES}
    \end{axis}
  \end{tikzpicture}
  \caption[Benchmark linear solvers on GPU.]{Benchmarking comparing linear solvers on the GPU. The direct solver is provided by MAGMA,~\cite{MagmaDocumentation}, and iterative solvers, BiCGStab and GMRES, by ViennaCL,~\cite{ViennaCLRupp2010}.}
  \label{fig:bench_cuda_solvers}
\end{figure}

The fact that iterative solvers are faster than a direct solver holds true also for the GPU. However, compared to the results on the CPU the difference between the performance of the direct solver and the iterative solvers is not as large. At close to $2000$ fibers the iterative solvers are only about $25×$ faster than the direct solver as compared to the result on the CPU where GMRES was $300×$ faster than the direct solver. Looking at the difference between the two iterative solvers, BiCGStab and GMRES they perform almost exactly the same. Any small differences can be attributed to small measuring uncertainties. 

Regardless of these clear results, it is always important to keep in mind, that this particular performance ratio only holds true for the specific fiber concentration which was benchmarked. For other concentrations the iterative solvers might need more iterations to find the solution and might even perform worse than the linear solver.

\section{Individual steps of the algorithm}

The next benchmark explores the relative time taken by each step of the algorithm described in Sec.~\ref{sec:algorithm_summary}~and Sec.~\ref{sec:kernels}. This yields valuable insight into the time allocation of the overall execution time, thereby helping to figure out which steps are the best ones to optimize. The results for the OpenMP implementation can be seen in Fig.~\ref{fig:bench_openmp_steps} and for the CUDA implementation in Fig.~\ref{fig:bench_cuda_steps}. On the CPU the \emph{Assemble System} and \emph{Update Velocities} steps solve the inner integral in Eqn.~\eqref{eq:inner_integral} analytically. On the GPU we use a purely numerical approach. The reason for choosing different approaches for evaluating the inner integral is to use the fastest version of the algorithm on both the CPU and GPU, see Sec.~\ref{subsec:bench_numeric_vs_analytic}.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
	width=0.77\textwidth,
      xlabel={Number of fibers},
      ylabel={Execution time (sec)},
      stack plots=y,
      ymin=0,ymax=50,
      xmin=0,xmax=2000,
      ]
      \addplot[color=set19,mark=*,mark options={fill=white}, very thick, stack plots=false] table[x=X,y=TOTAL] {benchmarks/openmp_gmres_analytical.csv};
      \addplot[color=set11,fill=set11_light,ultra thin,area style] table[x=X,y=ASSEMBLE_SYSTEM] {benchmarks/openmp_gmres_analytical.csv} \closedcycle;
      \addplot[color=set12,fill=set12_light,ultra thin,area style] table[x=X,y=SOLVE_SYSTEM]{benchmarks/openmp_gmres_analytical.csv} \closedcycle;
      \addplot[color=set13,fill=set13_light,ultra thin,area style] table[x=X,y=UPDATE_VELOCITIES] {benchmarks/openmp_gmres_analytical.csv} \closedcycle;
      \addplot[color=set14,fill=set14_light,ultra thin,area style] table[x=X,y=UPDATE_FIBERS] {benchmarks/openmp_gmres_analytical.csv} \closedcycle;

      \legend{Total, Assemble System, Solve System, Update Velocities, Update Fibers}
    \end{axis}
  \end{tikzpicture}
  \caption[Benchmark individual steps on CPU.]{Benchmark comparing the execution for each individual step of the algorithm using the OpenMP-based CPU implementation.}
  \label{fig:bench_openmp_steps}

  \begin{tikzpicture}
    \begin{axis}[
	width=0.77\textwidth,
      xlabel={Number of fibers},
      ylabel={Simulation time (sec)},
      stack plots=y,
      ymin=0,ymax=1.4,
      xmin=0,xmax=2000,
      ]
      \addplot[color=set19,mark=*,mark options={fill=white}, very thick, stack plots=false] table[x=X,y=total] {benchmarks/cuda_gmres_numerical_2D.csv};
      \addplot[color=set11,fill=set11_light,ultra thin,area style] table[x=X,y=assemble_system] {benchmarks/cuda_gmres_numerical_2D.csv} \closedcycle;
      \addplot[color=set12,fill=set12_light,ultra thin,area style] table[x=X,y=solve_system]{benchmarks/cuda_gmres_numerical_2D.csv} \closedcycle;
      \addplot[color=set13,fill=set13_light,ultra thin,area style] table[x=X,y=update_velocities] {benchmarks/cuda_gmres_numerical_2D.csv} \closedcycle;
      \addplot[color=set14,fill=set14_light,ultra thin,area style] table[x=X,y=update_fibers] {benchmarks/cuda_gmres_numerical_2D.csv} \closedcycle;

      \legend{Total, Assemble System, Solve System, Update Velocities, Update Fibers}
    \end{axis}
  \end{tikzpicture}
  \caption[Benchmark individual steps on GPU.]{Benchmark comparing the execution for each individual step of the algorithm using the CUDA-based GPU implementation.}
  \label{fig:bench_cuda_steps}
\end{figure}

The results for the OpenMP implementation, Fig.~\ref{fig:bench_openmp_steps}, show that for $2000$ fibers the \emph{Assemble System} step is responsible for $78\%$ and the \emph{Update Velocities} step for $21\%$ of the overall time. The two other steps \emph{Solve System} and \emph{Update Fibers} barely register with just $1\%$. However, as discussed in the previous Sec.~\ref{sec:bench_linear_solvers} this only holds true for sufficiently low concentrations of fibers.

\begin{figure}[htbp]
  \centering

\end{figure}

For the CUDA implementation, Fig.~\ref{fig:bench_cuda_steps} shows that the \emph{Assemble System} step remains the largest block with $72\%$ of the time. However, now the \emph{Solve System} step takes $20\%$ of the time and \emph{Update Velocites} drops to just $7\%$. The \emph{Update Fibers} step is also negligible here with below $1\%$.

This difference between the CPU and GPU implementation in the relative time between the steps can be attributed to the comparatively slow GMRES implementation on the GPU. In absolute terms for $2000$ fibers both the CPU and GPU take ${\sim}0.25$ seconds for solving the system. But, when transferring the same relative distribution from the CPU to GPU this makes the GPU GMRES implementation $40×$ too slow. The other steps all perform relatively better.

The reason for this most likely lies in the non-optimized code for the GMRES solver in ViennaCL. The authors openly state that the major goal for their library is easy of use and not pure performance,~\cite{ViennaCLRupp2010}. The parallel BLAS functions from OpenBLAS on the other hand have been highly optimized and tested for a long time.

This point is reinforced if we look at the same timings for the linear solvers. Here the absolute time for \emph{Solve System} step on the GPU is just ${\sim}7$ seconds compared to the ${\sim}76$ seconds on the CPU. Again transferring the relative distribution from the CPU to the GPU yields just a small factor of ${\sim}1.4×$. This illustrates that the highly optimized code of the MAGMA library performs roughly on the same level as the optimized code from OpenBLAS.

\section{GPU vs. CPU}

The final benchmark compares the CPU and GPU performance. How to make a fair comparison of the simulation performance between the CPU and GPU is a hotly debated topic in the research literature,~\cite{Gregg2011}\cite{Lee2010}. The underlying architectures of the two approaches are completely different and thus hard to compare. There are approaches where the relative performance is extrapolated from the underlying FLOPs by taking processor count, frequency and memory bandwidth into account,~\cite{Lee2010}. However, due to intricate hardware details this approach is not applicable to all scenarios. Thus in the super computing community metrics like performance-per-dollar or even performance-per-watt have become the main focus,~\cite{Kamil2008}.

Exploring this question in more detail is out of the scope of this thesis. Nevertheless we try to make a best effort to do fair comparison between the CPU and the GPU performance. In order to come as close as possible given these complexities and constraints, we use a modern CPU and GPU which can be considered a balanced system at the time of writing. Additionally, we implemented a parallel OpenMP version. It is directly based on the parallel CUDA version with the sole purpose to have as few difference between the two implementations as possible. Furthermore, the final benchmark uses the fastest possible version of the algorithms as determined by the performed benchmarks.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      xlabel={Number of fibers},
      ylabel={Execution time (sec)},
      enlarge y limits=true,
      enlarge x limits=true,
      unbounded coords=discard,
      xmode=log,
      ymode=log,
      grid=major,
      ]

      \addplot[color=set11,mark=*,mark options={fill=white}, very thick] table[x=X,y=total] {benchmarks/cuda_gmres_numerical_2D.csv};
      \addplot[color=set11_light,mark=square*,mark options={fill=white}, very thick] table[x=X,y=total] {benchmarks/cuda_magma_numerical_2D.csv};
      \addplot[color=set12,mark=triangle*,mark options={fill=white}, very thick] table[x=X,y=TOTAL] {benchmarks/openmp_gmres_analytical.csv};
      \addplot[color=set12_light,mark=diamond*,mark options={fill=white}, very thick] table[x=X,y=TOTAL] {benchmarks/openmp_direct_analytical.csv};

      \legend{CUDA GMRES, CUDA Direct, OpenMP GMRES, OpenMP Direct}
    \end{axis}
  \end{tikzpicture}
  \caption[Benchmark overall execution time.]{Benchmark comparing overall execution time for OpenMP and CUDA. Both the CPU and GPU implementation are run using the fastest algorithm as determined in Sec~\ref{sec:bench_optimization} and each implementation is tested with a direct and iterative solver. The CUDA-based GPU implementation outperforms the OpenMP-based CPU implementation by ${\sim}40×$.\looseness=-1}
  \label{fig:overall}
\end{figure}

The results for the average time required to take a single time step is illustrated in Fig.~\ref{fig:overall}. For OpenMP the algorithm uses the analytical integration of the inner integral. For CUDA, we used the numerical integration and the thread block dimension was chosen to be two-dimensional.

The required simulation time on the GPU outperforms the CPU by a wide margin. The GPU version is faster for any number of fibers that we are able to test. The speedup factors for the tested versions are listed in Tab~\ref{tab:overall_speedup}. CUDA maintains a relative performance of ${\sim}40×$. The only advantage the CPU version has is the potentially larger memory as 4GB on the GPU limits the number of fibers to roughly $2000$. So for more fibers than $2000$, OpenMP is currently the only option.

\begin{table}[htbp]
  \begin{center}
    \begin{tabulary}{\textwidth}{lRRRR}
      \toprule
      M = 2000 & OpenMP Direct & OpenMP GMRES & CUDA Direct & CUDA GMRES \\
      \midrule
      OpenMP Direct & $1×$  & $—$   & $—$ & $—$ \\
      OpenMP GMRES  & $3×$  & $1×$  & $—$ & $—$ \\
      CUDA Direct   & $16×$ & $6×$  & $1×$ & $—$ \\
      CUDA GMRES    & $99×$ & $39×$ & $6×$ & $1×$ \\
      \bottomrule
    \end{tabulary}
  \end{center}
  \caption[Speedup factors for overall execution time.]{The speedup factors for the overall execution time of a simulation with $2000$ fibers for the CPU and GPU implementation. In case of a direct solver the GPU implementation shows a speedup of $16×$ compared to the CPU implementation. Comparing the two GMRES versions, the CUDA-based GPU implementation outperforms the OpenMP-based CPU implementation by $39×$.}
  \label{tab:overall_speedup}
\end{table}

We acknowledge that these numbers and performance increases are not necessarily fair. A different CPU and GPU combination from the one used in this thesis might perform differently. However, the relative performance should stay roughly the same. In the end, the only thing that really matters for the researcher working with rigid fibers is the time it takes to simulate large systems on the available workstation. There is no need to wait for computing time at a large computing cluster. Instead simulation can be run simply on a desktop computer allowing to rapidly iterate on the tests. The observed performance increase of $40×$ is comparable to a difference between a whole day of waiting for the simulation results and a quick $30$ minutes result. The saved time also translates directly into the ability to simulate many more fibers than before. Using the original serial implementation the largest simulation possible in a reasonable time frame where around $500$ to $800$ fibers. Now one single time step with $2000$ fibers takes at most $8$ seconds on the GPU. Therefore the implementation on the GPU and the optimizations of the simulation code is of great value to the research of rigid fiber simulations.
