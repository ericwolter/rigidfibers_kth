\chapter{Conclusion}

In this thesis we have developed a completely new parallel GPU implementation for the numerical simulation of rigid fibers suspension using CUDA. This is faster and therefore many more fibers can be simulated than before. Furthermore being able to simulate more fibers enables the researcher to perform more extensive and in-depth studies of the various properties of the flow and simulation.

Based on the theoretical foundation of the simulated model and the original serial Fortran implementation we rewrote the algorithm to take advantage of massively parallel computational power available in modern GPUs. We outlined the required steps to implement the algorithm using CUDA. We investigated a number of different optimizations to further improve the performance. These explored optimizations included variations of the algorithm, GPU specific implementation details and the choice of linear solver. To reach the goal of comparing the performance between CPU and GPU the new parallel algorithm was backported to the CPU using OpenMP. Thereby improving the fairness of the comparison as much as possible.

During extensive benchmarking we showed that variations perform differently depending on the underlying architecture. Optimizations developed for the CPU implementation actually slow down the GPU implementation due to diverging execution branches. We also discovered that off-the-shelf GPU implementations of iterative solvers can be slower compared to their highly optimized CPU alternatives. Hopefully future research in this area will allow for even better performance of iterative solvers on the GPU.

Overall our GPU simulation on the NVIDIA GTX 970 is able to outperform the CPU version on an Intel Core i7 4770 by a factor of $20×$ to $40×$. This allows the GPU simulation to handle up to $2000$ fibers, which is more than double then before, and only takes $8$ seconds to advance one time step on a desktop computer.

To test and validate our new parallel implementation, we performed a number of simulations of known experiments for sedimenting fibers. Using the test-case of tumbling orbits we showed that the numerical precision of our single precision implementation is able to reproduce the result obtained from the former double precision Fortran simulations. We also explored how the performance of iterative solvers is affected by the fiber concentration, showing that if fibers come too close the number of iterations increases and slows down the simulation. Our last experiment looked at the known phenomena that a sphere of fibers sedimenting form a torus and eventually break up into smaller clusters. Our results show an excellent agreement with both experimental and numerical work performed previously. Additionally, we showed that the stability of the torus is positively correlated with the increasing distance between fibers and the total number of fibers. Further research in this area is helped greatly by the fact that new case can rapidly be explored using our fast GPU simulation.

Currently the number of fibers in our simulation is limited by the amount of memory available on the GPU. In future we would like to research how to lift this restriction. One possible research direction is to avoid storing the matrix completely. Instead the required computation would have to be carried out iteratively and on-demand. This would allow for a practical unlimited number of fibers with the cost of an largely increased computation time. If this trade off is worth it remains to be seen.
Figure
Another exciting possibility is the move to a multi-device setup. Using multiple GPUs simultaneously would enable much larger systems, if the workload can be split efficiently. MAGMA, the library used for the GPU direct solver, offers a couple of interesting options in this area. The most challenging part to efficient parallelization is solving the system and broadcasting the result across multiple device. The other steps of the algorithm can be naturally partitioned and have already been implemented across multiple CPUs using OpenMPI.

Finally, to further improve the simulation it might be worth looking at the numerical algorithm itself. Adapting a different algorithm based on the fast summation method might not only improve the performance but also reduce the required memory allowing for faster and larger simulation. How this affects the accuracy and behavior of the simulation is an interesting future research question.
