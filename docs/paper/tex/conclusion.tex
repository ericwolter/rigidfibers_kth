\chapter{Conclusion}

In this thesis we have developed a completely new parallel GPU implementation, using CUDA, for the numerical simulation of rigid fibers suspension. The new GPU implementation of the numerical algorithm outperforms the older CPU version with a speed up of $20×$ to $40×$. This enables simulations with many more fibers than before. Being able to simulate more fibers enables the researcher to perform more extensive and in-depth studies of the various properties of the flow and simulation.

Based on the theoretical foundation of the simulated model and the original serial Fortran implementation we developed an algorithm suitable for taking advantage of massively parallel computational power available in modern GPUs. We outline the required steps to implement the algorithm using CUDA and investigate a number of different optimization strategies to further improve the performance. Both versions of the algorithm, GPU specific implementation details and the choice of linear solver are investigated. To reach the goal of comparing the performance between the CPU and GPU implementation the new parallel algorithm is backported to the CPU using OpenMP. Thereby improving the fairness of the comparison as much as possible.

Using extensive benchmarking we show that different versions of the two implementations perform differently depending on the underlying architecture. E.g. optimizations developed for the CPU implementation actually slows down the GPU implementation due to diverging execution branches. We also discovered that off-the-shelf GPU implementations of iterative solvers can be slower compared to their highly optimized CPU alternatives. Hopefully future research in this area will allow for even better performance of iterative solvers on the GPU.

To test and validate our new parallel implementation, we perform a number of simulations of known experiments for sedimenting fibers. Using the test-case of tumbling orbits we show that the numerical precision of our single precision implementation is able to reproduce the result obtained from the former double precision Fortran simulations. As an example of the ability to handle many fibers, simulations with a sedimenting spherical cloud of fiber are performed. This setup displays a very interesting physical behavior. Once the fibers start to sediment, the spherical cloud is transformed into a torus that eventually breaks up into smaller cloudlets. Our results show an excellent agreement with both experimental and numerical work performed previously. Additionally, we show that the stability of the torus, that is the time until break-up, is positively correlated with the increasing distance between fibers as well as the total number of fibers. Finally, we look at the sedimenting behavior of a mixed density spherical cloud of fibers. In contrast to the uniform density spherical cloud the two groups of different density fibers separate and form independent tori. These tori undergo complex interactions and eventually break-up. Further research in this area is helped greatly by the fact that new cases can rapidly be explored using our fast GPU simulation.\looseness=-1

Currently the number of fibers in our simulation is limited by the amount of memory available on the GPU. In the future we would like to explore how to get around this restriction. One possible direction is to avoid storing the matrix completely. Instead the required computation will be carried out iteratively and on-demand. Concerning the memory, this will allow for an unlimited number of fibers, but coming at the cost of a largely increased computational time. If this trade off is worth it remains to be seen.

Another exciting possibility is to use a multi-device setup. Using multiple GPUs simultaneously would enable much larger systems, if the workload can be split efficiently. MAGMA, the library used for the GPU direct solver, offers a couple of interesting options in this area. The most challenging part to efficient parallelization is solving the system and broadcasting the result across multiple device. The other steps of the algorithm can be naturally partitioned and have already been implemented across multiple CPUs using OpenMPI.
\enlargethispage{\baselineskip}
Finally, to further improve the efficiency it might be worth looking at the numerical algorithm itself. Adapting a different algorithm based on a fast summation method might not only improve the performance but also reduce the required memory allowing for faster and larger simulation. How this affects the accuracy and behavior of the simulation is an interesting future research question.
